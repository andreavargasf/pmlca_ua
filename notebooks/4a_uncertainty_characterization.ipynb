{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7c9bdd-77d5-4c0b-b921-97bb9cd6f1d5",
   "metadata": {},
   "source": [
    "# Uncertainty Characterization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5e2c4-6d9c-4b29-bb79-30a549e3db6f",
   "metadata": {},
   "source": [
    "This notebook contains the required steps to characterize uncertainty in the foreground parameters considered in the analysis. We use probability density functions (PDFs) to characterize uncertainties. \n",
    "\n",
    "The uncertainty characterization values originally come as an [excel file](Uncertainties.xlsx). To use the code presented in this notebook, one must adhere to the column format of the excel file. \n",
    "\n",
    "All foreground input parameters, i.e., input quantities describing modules A1 to C3, are threated stochastically and characterized using PDFs. These are defined following the [ecoinvent method](https://vbn.aau.dk/ws/files/176769045/overview_and_methodology.pdf) and are presented in the table below:\n",
    "\n",
    "| Phase | Input parameter | Unit | Uncertainty value | Mean—0% RAP | Mean—30% RAP | Distribution |\n",
    "|-------|-----------------|------|-------------------|--------------|---------------|-------------|\n",
    "| **A1** |                 |      |                   |              |               |             |\n",
    "|       | Bitumen         | kg   | 0.0012            | 52.00        | 41.20         | lognormal   |\n",
    "|       | Crushed sand    | kg   | 0.0012            | 43.00        | 34.20         | lognormal   |\n",
    "|       | Crushed stone 3*| kg   | 0.0012            | 852.00       | 586.10        | lognormal   |\n",
    "|       | Asphalt granulate—RAP | kg | 0.0012       | 0.00         | 300.00        | lognormal   |\n",
    "|       | Own material | kg   | 0.0012            | 0.00         | 9.40          | lognormal   |\n",
    "|       | Medium filler   | kg   | 0.0012            | 51.00        | 27.00         | lognormal   |\n",
    "|       | Drip resistant material | kg | 0.0012       | 2.00         | 2.10          | lognormal   |\n",
    "| **A2** |                 |      |                   |              |               |             |\n",
    "|       | Bitumen - truck | ton-km   | 0.1207            | 250.00       | 250.00        | lognormal   |\n",
    "|       | Crushed sand - truck | ton-km | 0.1207         | 25.00        | 25.00         | lognormal   |\n",
    "|       | Crushed sand - inland vessel | ton-km | 0.1207 | 660.00       | 660.00        | lognormal   |\n",
    "|       | Crushed stone 3* - truck | ton-km | 0.1207     | 25.00        | 25.00         | lognormal   |\n",
    "|       | Crushed stone 3* - inland vessel | ton-km | 0.1207 | 53.00     | 53.00         | lognormal   |\n",
    "|       | Crushed stone 3* - sea vessel | ton-km | 0.1207 | 933.00       | 933.00        | lognormal   |\n",
    "|       | Own material – truck | ton-km   | 0.1207        | 0.00         | 25.00         | lognormal   |\n",
    "|       | Own material - inland vessel | ton-km | 0.1207    | 0.00         | 150.00        | lognormal   |\n",
    "|       | Medium filler - truck | ton-km  | 0.1207         | 136.00       | 136.00        | lognormal   |\n",
    "|       | Drip resistant material - truck | ton-km | 0.1207 | 177.00       | 177.00        | lognormal   |\n",
    "| **A3** |                 |      |                   |              |               |             |\n",
    "|       | Natural gas     | m^3  | 0.0012            | 7.43         | 8             | lognormal   |\n",
    "|       | Electricity     | kWh  | 0.0012            | 6.23         | 5.61          | lognormal   |\n",
    "|       | Diesel          | l    | 0.0012            | 0.12         | 0.12          | lognormal   |\n",
    "| **A4** |                 |      |                   |              |               |             |\n",
    "|       | Distance to construction site | km | 0.1207 | 44.40        | 44.4          | lognormal   |\n",
    "| **A5** |                 |      |                   |              |               |             |\n",
    "|       | Asphalt set (spreader + roller) | l | 0.0014 | 0.32         | 0.32          | lognormal   |\n",
    "| **B1** |                 |      |                   |              |               |             |\n",
    "|       | Passenger car   | l    | 46736.19          | 68967.84     | 68967.84      | normal      |\n",
    "|       | Heavy Duty Vehicle (HDV) | l | 10534.82      | 17729.00     | 17729         | normal      |\n",
    "|       | HDV + trailer   | l    | 41990.32          | 58536.78     | 58536.78      | normal      |\n",
    "| **C1** |                 |      |                   |              |               |             |\n",
    "|       | Milling + cleaning + sweeping | l | 0.0014  | 0.77         | 0.77          | log\n",
    "\n",
    "<small>Note: Values are given for 1 ton of asphalt and scaled to the FU in the analysis considering an asphalt mixture density of 2000 kg/m3.</small>\n",
    "\n",
    "<small>*According to the NL-PCR, crushed stone 3 refers to coarse aggregates obtained from a quarry using explosives rather than from a river/lake obtained via excavation and crushing. </small>\n",
    "\n",
    "A brief explanation on how these values were calculated is provided hereafter. For more information, refer to the case study background material presented in the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefbe376-dbff-45f6-84e1-d778ea413dea",
   "metadata": {},
   "source": [
    "## Parameter Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c283f-cd9d-4f6f-81bf-f6b36aab5202",
   "metadata": {},
   "source": [
    "The ecoinvent method characterizes parameter uncertainty in terms of basic uncertainty (i.e., data variability) and additional uncertainty (i.e., data quality), asumming that the PDFs characterizing each parameter follow a long-normal distribution using the following formula:\n",
    "\n",
    "$$\\sigma^2 = \\sigma_b^2 + \\sum_{n=1}^{5} \\sigma_n^2$$\n",
    " \n",
    "Where all terms are expressed as variance of the log-transformed data, i.e., the underlying normal distribution with $\\sigma_n^2$ representing total parameter uncertainty; $\\sigma_b^2$, basic uncertainty or data variability, and $\\sum_{n=1}^{5} \\sigma_n^2$, additional uncertainty due to each data quality dimension included in the assessment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5193fd2-ac2c-4bed-bca5-3f1c43a29754",
   "metadata": {},
   "source": [
    "### Basic Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88f298-6f1e-4463-b622-28cd5b4affee",
   "metadata": {},
   "source": [
    "To characterize basic uncertainty, it's best to use empirical data to build samples and PDFs. This data can be directly collected from contractors or suppliers, or indirectly through EPDs and previous projects. However, acquiring and defining basic uncertainty empirically can be time and resource-intensive. While using empirical values is recommended when possible, default uncertainty values from ecoinvent, below presented, can serve as an alternative when empirical data isn't accessible.\n",
    "\n",
    "<small>*Ecoinvent default basic uncertainty values (variances of the underlying normal distributions $\\sigma_b^2$) for different types of (intermediate and elementary) exchanges.*</small>\n",
    "\n",
    "| Input/Output Group                                                     | c     | p    | a     |\n",
    "|------------------------------------------------------------------------|-------|------|-------|\n",
    "| **Demand of:**                                                         |       |      |       |\n",
    "| Thermal energy, electricity, semi-finished products, working material, waste treatment services | 0.0006| 0.0006| 0.0006|\n",
    "| Transport services (tkm)                                               | 0.12  | 0.12 | 0.12  |\n",
    "| Infrastructure                                                         | 0.3   | 0.3  | 0.3   |\n",
    "| **Resources:**                                                         |       |      |       |\n",
    "| Primary energy carriers, metals, salts                                 | 0.0006| 0.0006| 0.0006|\n",
    "| Land use, occupation                                                   | 0.04  | 0.04 | 0.002 |\n",
    "| Land use, transformation                                               | 0.12  | 0.12 | 0.008 |\n",
    "| **Pollutants emitted to water:**                                       |       |      |       |\n",
    "| BOD, COD, DOC, TOC, inorganic compounds (NH4, PO4, NO3, Cl, Na etc.)   | -     | 0.04 | -     |\n",
    "| Individual hydrocarbons, PAH                                           | -     | 0.3  | -     |\n",
    "| Heavy metals                                                           | -     | 0.65 | 0.09  |\n",
    "| Pesticides                                                             | -     | -    | 0.04  |\n",
    "| NO3, PO4                                                               | -     | -    | 0.04  |\n",
    "| **Pollutants emitted to soil:**                                        |       |      |       |\n",
    "| Oil, hydrocarbon total                                                 | -     | 0.04 | -     |\n",
    "| Heavy metals                                                           | -     | 0.04 | 0.04  |\n",
    "| Pesticides                                                             | -     | -    | 0.033 |\n",
    "| **Pollutants emitted to air:**                                         |       |      |       |\n",
    "| CO2                                                                    | 0.0006| 0.0006| -     |\n",
    "| SO2                                                                    | 0.0006| -    | -     |\n",
    "| NMVOC total                                                            | 0.04  | -    | -     |\n",
    "| NOX, N2O                                                               | 0.04  | -    | 0.3   |\n",
    "| CH4, NH3                                                               | 0.04  | -    | 0.008 |\n",
    "| Individual hydrocarbons                                                | 0.04  | 0.12 | -     |\n",
    "| PM>10                                                                  | 0.04  | 0.04 | -     |\n",
    "| PM10                                                                   | 0.12  | 0.12 | -     |\n",
    "| PM2.5                                                                  | 0.3   | 0.3  | -     |\n",
    "| Polycyclic aromatic hydrocarbons (PAH)                                 | 0.3   | -    | -     |\n",
    "| CO, heavy metals                                                      | 0.65  | -    | -     |\n",
    "| Inorganic emissions, others                                            | -     | 0.4  | -     |\n",
    "| Radionuclides (e.g., Radon-222)                                       | -     | 0.3  | -     |\n",
    "\n",
    "When basic uncertainty is empirically calculated and does not conform to a log-normal distribution, researchers can adopt the methodology delineated by [Muller et al.,](https://link.springer.com/article/10.1007/s11367-014-0759-5) to add basic and additional uncertainties together and determine total parameter uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370529ad-f07f-4ac9-8da0-0ba46bae737d",
   "metadata": {},
   "source": [
    "### Additional Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7ec30-7ff1-472d-a30a-5c8594fed910",
   "metadata": {},
   "source": [
    "Additional uncertainty is assessed using the pedigree matrix approach from the ecoinvent method. Initially, empirical quantity values of each input parameter are evaluated based on quality indicators (DQIs). DQIs, scored from 1 to 5, reflect the uncertainty level of due to data quality, assessed in terms of reliability, completeness, temporal correlation, geographical correlation, and further technological correlation, as showed in the table below. \n",
    "\n",
    "| DQIs / DQI Score    | 1                                                    | 2                                                                     | 3                                                                           | 4                                                                               | 5 (Default)                                                               |\n",
    "|----------------------|------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| Reliability          | Verified data based on measurements                 | Verified data partly based on assumptions or non-verified data based on measurements | Non-verified data partly based on qualified estimates                    | Qualified estimate (e.g., by industrial expert)                               | Non-qualified estimate                                                    |\n",
    "| Completeness         | Representative data from all sites relevant for the market considered, over an adequate period to even out normal fluctuations | Representative data from >50% of the sites relevant for the market considered, over an adequate period to even out normal fluctuations | Representative data from only some sites (<<50%) relevant for the market considered or >50% of sites but from shorter periods | Representative data from only one site relevant for the market considered or some sites but from shorter periods | Representativeness unknown or data from a small number of sites and from shorter periods |\n",
    "| Temporal correlation| Less than 3 years of difference to the time period of the dataset | Less than 6 years of difference to the time period of the dataset     | Less than 10 years of difference to the time period of the dataset       | Less than 15 years of difference to the time period of the dataset           | Age of data unknown or more than 15 years of difference to the time period of the dataset |\n",
    "| Geographical correlation | Data from area under study                           | Average data from larger area in which the area under study is included | Data from area with similar production conditions                        | Data from area with slightly similar production conditions                   | Data from unknown or distinctly different area (North America instead of Middle East, OECD-Europe instead of Russia) |\n",
    "| Further technological correlation | Data from enterprises, processes, and materials under study | Data from processes and materials under study (i.e., identical technology) but from different enterprises | Data from processes and materials under study but from different technology | Data on related processes or materials                                     | Data on related processes on laboratory scale or from different technology |\n",
    "\n",
    "These scores are then transformed into additional uncertainty values using specific factors detailed in the subsequent table, and further added to basic uncertainty to quantify overall parameter uncertainty. \n",
    "\n",
    "| Indicator Score               | 1       | 2          | 3        | 4      | 5      |\n",
    "|-------------------------------|---------|------------|----------|--------|--------|\n",
    "| Reliability                  | 0       | 0.0006     | 0.002    | 0.008  | 0.04   |\n",
    "| Completeness                 | 0       | 0.0001     | 0.0006   | 0.002  | 0.008  |\n",
    "| Temporal correlation         | 0       | 0.0002     | 0.002    | 0.008  | 0.04   |\n",
    "| Geographical correlation     | 0       | .000025 | 0.0001   | 0.0006 | 0.002  |\n",
    "| Further technological correlation | 0  | 0.0006     | 0.008    | 0.04   | 0.12   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afe050-2a3e-49ff-8462-d38d3105410a",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Samples are generated using the [SALib](https://salib.readthedocs.io/en/latest/) Python library using the following convention:\n",
    "\n",
    "| Distribution | Bounds            | Examples         | Comments                                                           |\n",
    "|--------------|-------------------|------------------|--------------------------------------------------------------------|\n",
    "| unif         | [lower, upper]   | [-3.14159, 3.14159] | Uniform distribution with bounds from -π to π                     |\n",
    "| logunif      | [lower, upper]    | [0.1, 10]        | Logarithmic uniform with bounds from 0.1 to 10                    |\n",
    "| triang       | [lower, upper, peak] | [1.0, 3.0, 0.5] | Triangular with bounds from 1.0 to 3.0 and a peak at 2.0          |\n",
    "| norm         | (mean, std_dev)   | (0, 1)           | Normal distribution with mean 0 and standard deviation 1          |\n",
    "| truncnorm    | [lower, upper, mean, std_dev] | [0, 1, 0, 1] | Truncated normal with bounds from 0 to 1, mean 0, and standard deviation 1 |\n",
    "| lognorm      | (ln_mean, ln_std_dev) | (0, 1)        | Lognormal with ln-space mean 0 and ln-space standard deviation 1  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60df6880-d89b-45b3-bb57-bfc07485f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from SALib.sample import latin, sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d09a1908-4998-4b40-9502-d714642f963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we add the path to the excel file that contains the parameter uncertainty characterization values\n",
    "input_path = ('Uncertainties.xlsx')\n",
    "\n",
    "#then we add the download path to store our samples\n",
    "download_path = ('Samples.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "179834f7-7787-40cf-86b0-feb5a3cf348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define a sampling function that draws from the excel file \n",
    "\n",
    "def sampling_generator(pavement_system, sampling_method, sample_size, random_seed):\n",
    "       '''\n",
    "    Parameters:\n",
    "    \n",
    "    pavement system (string) : name of the pavement system/scenario to be sampled as named in the corresponding excel sheet\n",
    "    sampling_method (string): LHS (Latin hypercube sampling) or Sobol, depending on the GSA method to be applied later on. \n",
    "    sample_size (int) : For LHS, number of samples to be generated; for Sobol, baseline sample N in N * (D + 2) \n",
    "                                    where ``D`` is the number of parameters\n",
    "    random_seed (int): seed use to generate samples                            \n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    input_df (excel) : Excel containing a dataframe with the generated samples\n",
    "    \n",
    "    '''\n",
    "    #warnings.filterwarnings(\"ignore\")\n",
    "    #We extract the names, distributions and sample-specific metadata from the excel file required for sampling    \n",
    "    parameters = pd.DataFrame(pd.read_excel\n",
    "                        (input_path,\n",
    "                        sheet_name=pavement_system)) #sheet containing the values of the pavement system (scenario) to sample\n",
    "\n",
    "    parameters = parameters.round(4)\n",
    "    \n",
    "    #we set the bounds to define PDFs according to SALib's convention\n",
    "    parameters['bounds'] = ''             \n",
    "    for index in range(len(parameters)):\n",
    "        if parameters['dist'].loc[index] == 'lognorm':\n",
    "            parameters['bounds'].loc[index] = [parameters['mean'].loc[index],\n",
    "                                               parameters['std'].loc[index]]\n",
    "        elif parameters['dist'].loc[index] in ['unif', 'logunif']:\n",
    "            parameters['bounds'].loc[index] = [parameters['lower bound'].loc[index],\n",
    "                                               parameters['upper bound'].loc[index]]\n",
    "        elif parameters['dist'].loc[index] == 'triang':\n",
    "            parameters['bounds'].loc[index] = [parameters['lower bound'].loc[index],\n",
    "                                               parameters['upper bound'].loc[index],\n",
    "                                               parameters['mean'].loc[index]]\n",
    "        elif parameters['dist'].loc[index] == 'norm':\n",
    "            parameters['bounds'].loc[index] = [parameters['mean'].loc[index],\n",
    "                                               parameters['std'].loc[index]] \n",
    "        else:\n",
    "            parameters['bounds'].loc[index] = [parameters['lower bound'].loc[index],\n",
    "                                               parameters['upper bound'].loc[index],\n",
    "                                               parameters['mean'].loc[index],\n",
    "                                               parameters['std'].loc[index]]\n",
    "            \n",
    "    parameters = parameters.set_index('name')\n",
    "    params = parameters['bounds'].squeeze()\n",
    "    dists = parameters['dist'].squeeze() #we extract the distributions as a variable\n",
    "    #bounds=np.array([np.array(i) for i in params.values])\n",
    "    bounds=[i for i in params]\n",
    "    num_vars=len(bounds)\n",
    "\n",
    "    #SALib problem definition and sampling\n",
    "    problem={'num_vars':num_vars, 'bounds':bounds, 'names':params.index, 'dists':dists.values}\n",
    "    n=sample_size #Baseline sample size\n",
    "    if sampling_method == 'Sobol':\n",
    "        samples=sobol.sample(problem, N=n, calc_second_order=False, seed=random_seed)\n",
    "    elif sampling_method == 'LHS':\n",
    "        samples=latin.sample(problem, N=n, seed=random_seed)\n",
    "\n",
    "    input_df=pd.DataFrame(samples, #store samples as dataframe\n",
    "                        columns=params.index)\n",
    "\n",
    "    # Check if cdownload_path exists, and create it if it doesn't\n",
    "    if not os.path.exists(download_path):\n",
    "        # Create an empty Excel file\n",
    "        pd.DataFrame().to_excel(download_path)\n",
    "\n",
    "    #Save sample dataframe\n",
    "    with pd.ExcelWriter(download_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        input_df.to_excel(writer, sheet_name = pavement_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "534887a5-7f99-40a4-abae-3beecb016476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we run our sampling function for the different scenarios/pavement systems that we want to sample\n",
    "pavement_systems = ['DZOAB, B', 'DZOAB, B, PVI', 'DZOAB, A', 'DZOAB, A, PVI']\n",
    "\n",
    "for pavement_system in pavement_systems:\n",
    "     sampling_generator(pavement_system, 'LHS', 12000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186913d-de37-4c46-a23a-ca6f7e7affa2",
   "metadata": {},
   "source": [
    "### Correlating exchanges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6659c5f-7e87-420b-8872-172af652e201",
   "metadata": {},
   "source": [
    "The quantities describing the transport of materials in A2 are correlated (dependant) on the quantity of materials defined in A1. \n",
    "\n",
    "| Material quantities in A1 | Tag | Corresponding transport distances in A2 | Tag |\n",
    "|---------------------------|-----|-----------------------------------------|-----|\n",
    "| Bitumen                   |  A1_bitumen   | Bitumen - truck                         |   A2_bitumen  |\n",
    "| Crushed sand              |   A1_crushedsand  | Crushed sand - truck                    |  A2_crushedsand_t   |\n",
    "|                           |     | Crushed sand - inland vessel            |   A2_crushedsand_iv  |\n",
    "| Crushed stone             |  A1_crushedstone3   | Crushed stone - truck                   |  A2_crushedstone3_t   |\n",
    "|                           |     | Crushed stone - inland vessel           |   A2_crushedstone3_iv  |\n",
    "|                           |     | Crushed stone - sea vessel              |  A2_crushedstone3_sv   |\n",
    "| Own material              |  A1_ownmaterial   | Own material – truck                    |  A2_ownmaterial_t  |\n",
    "|                           |     | Own material – inland vessel                    |  A2_ownmaterial_iv  |\n",
    "| Medium filler             |  A1_mediumfiller   | Medium filler - truck                   |   A2_mediumfiller  |\n",
    "| Drip resistant material   |   A1_dripresistantmaterial  | Drip resistant material - truck         |   A2_dripresistantmaterial  |\n",
    "\n",
    "\n",
    "\n",
    "This translates into 8 dependant parameters for *DZOAB, B* scenarios, and 10 for *DZOAB, A* scenarios. \n",
    "\n",
    "In the process of generating samples for material quantities and transport distances, captured in the [samples](Samples.xlsx) Excel file generated in the previous step, it's essential to recognize the interdependency between these parameters. While the sampled distance values for A2 in the file provide insights into the noise affecting the total distance traveled for 1 kg of the respective material, they require further adjustment to reflect the actual quantities of materials being transported, as dictated by A1. These distance values, expressed in kg-km, signify the distance traveled for 1 kg of material, and thus need to be scaled accordingly. This entails multiplying the sampled distance values by the corresponding sampled material quantities from A1. By incorporating the uncertainties associated with both material quantities and transport distances, we ensure a more accurate representation of the relationship between these parameters and generate meaningful insights into the transportation dynamics within the sampled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e22da916-474d-48c3-99d9-790e2bcd25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the download path to store our correlated samples\n",
    "corr_download_path = ('Correlated_samples.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b605b9ca-3c57-42b4-9562-b3d1756937ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to correlate inputs\n",
    "def correlate_inputs(pavement_system):\n",
    "    '''Parameters:\n",
    "    pavement system (string) : name of the pavement system/scenario to be sampled as named in the corresponding excel sheet'''\n",
    "    # Create input_df including the samples of pavement_system selected\n",
    "    input_df = pd.DataFrame(pd.read_excel(download_path, \n",
    "                                          sheet_name=pavement_system, \n",
    "                                          index_col=0))\n",
    "    # Initialize empty DataFrame corr_input_df\n",
    "    corr_input_df = pd.DataFrame()\n",
    "    # Loop through column names of input_df\n",
    "    for column in input_df.columns:\n",
    "        # Check if column name contains \"A1\" or \"A2\"\n",
    "        if 'A1' in column or 'A2' in column:\n",
    "            # If yes, add the column to corr_input_df\n",
    "            corr_input_df[column] = input_df[column]\n",
    "            \n",
    "    # Create list with materials\n",
    "    materials = ['bitumen', 'crushedsand', 'crushedstone3', 'ownmaterial', 'mediumfiller', 'dripresistantmaterial']  \n",
    "    # Loop through the materials list      \n",
    "    for material in materials:\n",
    "        # Create a filter to select A1 columns that contain the material string\n",
    "        a1_columns = [col for col in corr_input_df.columns if col.startswith('A1') and material in col]\n",
    "        # Create a filter to select A2 columns that contain the material string\n",
    "        a2_columns = [col for col in corr_input_df.columns if col.startswith('A2') and material in col]\n",
    "        # Iterate over matched A1 columns\n",
    "        for a1_col in a1_columns:\n",
    "            for a2_col in a2_columns:\n",
    "                corr_input_df[a2_col] = corr_input_df[a2_col] * corr_input_df[a1_col]\n",
    "            \n",
    "    # Iterate over columns of corr_input_df\n",
    "    for column in corr_input_df.columns:\n",
    "        # Update values in input_df columns with the same name\n",
    "        input_df[column] = corr_input_df[column]\n",
    "\n",
    "    # Check if corr_download_path exists, and create it if it doesn't\n",
    "    if not os.path.exists(corr_download_path):\n",
    "        # Create an empty Excel file\n",
    "        pd.DataFrame().to_excel(corr_download_path)\n",
    "        \n",
    "    #Save correlated samples dataframe\n",
    "    with pd.ExcelWriter(corr_download_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        input_df.to_excel(writer, sheet_name = pavement_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aadec6ba-8f68-4b14-b2f6-7cbb3628d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate inputs for different pavement systems\n",
    "for pavement_system in pavement_systems:\n",
    "     correlate_inputs(pavement_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4494ff-f2dd-4482-93ce-f9a33d9ff0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
